{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My research question and Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My research question is \"How can bots be identified in social media comments?\".\n",
    "\n",
    "My corpus is list of comments from Twitter/X. \n",
    "\n",
    "I collected first 100 comments from Donald Trump's recent tweet https://x.com/realDonaldTrump/status/1894126415932526802/photo/1 as well as, got detailed information about comment authors. In addition, I got comments from russian media \"Meduza\" from the tweet https://x.com/meduzaproject/status/1894022390490837112 \n",
    "\n",
    "Comments collected by endpoint: https://api.twitter.com/2/tweets/search/recent\n",
    "\n",
    "UserInfo collected by endpoint: https://api.twitter.com/2/users?ids={batch_str}&user.fields=username,name,created_at,location,verified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data through Twitter API is very unconvineant if you use free API.\n",
    "Available pull size for tweets and retweets is just 100 per month.\n",
    "My first try was failed and I reached out my monthly limit.\n",
    "Collecting 160 retweets from 2 different tweets needed 3 different accounts from me)))\n",
    "\n",
    "I still have no idea how can I detect and which technicues I will use. However, I found repository with different datasets which I think will be helpfull if I will use supervised learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAABpFzgEAAAAAsPyyGIzE8c7x0o22RQvuwoHcars%3Djg0EW2CqUZ8rEduWfKUGJySMPayi4I0fke4YoJgDMNgKt1U67l\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tweet comments by conversation id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntweet_replies = get_tweet_replies(TWEET_ID)\\n\\n\\nwith open(FILE_PATH, \"w\", encoding=\"utf-8\") as file:\\n    json.dump(tweet_replies, file, ensure_ascii=False, indent=4)\\n\\nprint(f\"Data saved to {FILE_PATH}\")\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWEET_ID = \"1894126415932526802\"\n",
    "FILE_PATH = f\"data/tweet_info_{TWEET_ID}.json\"\n",
    "\n",
    "def get_tweet_replies(tweet_id):\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    headers = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\n",
    "    \n",
    "    all_replies = []\n",
    "    next_token = None\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"query\": f\"conversation_id:{tweet_id}\",\n",
    "            \"tweet.fields\": \"author_id,created_at\",\n",
    "            \"expansions\": \"author_id\",\n",
    "            \"user.fields\": \"username\",\n",
    "            \"max_results\": 100,\n",
    "        }\n",
    "\n",
    "        if next_token:\n",
    "            params[\"next_token\"] = next_token  \n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            with open(FILE_PATH+\"_all\", \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "            all_replies.extend(data.get(\"data\", []))\n",
    "\n",
    "            \n",
    "            next_token = data.get(\"meta\", {}).get(\"next_token\")\n",
    "            if not next_token:  \n",
    "                break  \n",
    "\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.json()}\")\n",
    "            break\n",
    "    \n",
    "    return all_replies\n",
    "\n",
    "'''\n",
    "tweet_replies = get_tweet_replies(TWEET_ID)\n",
    "\n",
    "\n",
    "with open(FILE_PATH, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(tweet_replies, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data saved to {FILE_PATH}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Users info by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique users: 84\n",
      "User data saved to data/user_info_1894126415932526802.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FILE_PATH = f\"data/tweet_info_{TWEET_ID}.json\"  \n",
    "FILE_PATH1 = f\"data/user_info_{TWEET_ID}.json\"  \n",
    "BATCH_SIZE = 100  \n",
    "SLEEP_TIME = 900  \n",
    "\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "users = list(set(tweet[\"author_id\"] for tweet in data))\n",
    "print(f\"Total unique users: {len(users)}\")\n",
    "\n",
    "\n",
    "def get_users_info(user_ids):\n",
    "    \"\"\"Fetches user information from Twitter in batches with 15-minute wait time.\"\"\"\n",
    "    all_users_info = []\n",
    "\n",
    "    for i in range(0, len(user_ids), BATCH_SIZE):\n",
    "        batch = user_ids[i : i + BATCH_SIZE]  \n",
    "        batch_str = \",\".join(batch)  \n",
    "\n",
    "        url = f\"https://api.twitter.com/2/users?ids={batch_str}&user.fields=username,name,created_at,location,verified\"\n",
    "        headers = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(FILE_PATH1+\"_all\", \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(response.json(), file, ensure_ascii=False, indent=4)\n",
    "            all_users_info.extend(response.json().get(\"data\", []))\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.json()}\")\n",
    "\n",
    "        if i + BATCH_SIZE < len(user_ids):  \n",
    "            print(\"Rate limit reached. Waiting 15 minutes before the next batch...\")\n",
    "            time.sleep(SLEEP_TIME)  \n",
    "\n",
    "    return all_users_info\n",
    "\n",
    "\n",
    "users_info = get_users_info(users)\n",
    "\n",
    "\n",
    "with open(FILE_PATH1, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(users_info, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"User data saved to {FILE_PATH1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
